{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import lit\n",
        "\n",
        "# Create a Spark session\n",
        "spark = SparkSession.builder.appName(\"AddConstantColumn\").getOrCreate()\n",
        "\n",
        "# Sample DataFrame\n",
        "data = [(\"Alice\", 25), (\"Bob\", 30), (\"Cathy\", 28)]\n",
        "columns = [\"Name\", \"Age\"]\n",
        "\n",
        "df = spark.createDataFrame(data, columns)\n",
        "\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6CW3ork7vcId",
        "outputId": "448adf3f-1adc-484b-ddcf-2f9c4d2206d6"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+---+\n",
            "| Name|Age|\n",
            "+-----+---+\n",
            "|Alice| 25|\n",
            "|  Bob| 30|\n",
            "|Cathy| 28|\n",
            "+-----+---+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Add a new column with a constant value\n",
        "df_with_constant = df.withColumn(\"Country\", lit(\"India\"))\n",
        "\n",
        "df_with_constant.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aGsFmZ6cxPZj",
        "outputId": "1fbf28cb-798c-4029-a46e-2f9355218b30"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+---+-------+\n",
            "| Name|Age|Country|\n",
            "+-----+---+-------+\n",
            "|Alice| 25|  India|\n",
            "|  Bob| 30|  India|\n",
            "|Cathy| 28|  India|\n",
            "+-----+---+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql import Row\n",
        "\n",
        "# Create a Spark session\n",
        "spark = SparkSession.builder.appName(\"AddColumnFromList\").getOrCreate()\n",
        "\n",
        "# Sample DataFrame\n",
        "data = [(\"Alice\", 25), (\"Bob\", 30), (\"Cathy\", 28)]\n",
        "columns = [\"Name\", \"Age\"]\n",
        "df = spark.createDataFrame(data, columns)\n",
        "\n",
        "# List of values to add as a new column\n",
        "country_list = [\"India\", \"China\", \"UK\"]\n",
        "\n",
        "# Align the list values with rows in the DataFrame using zipWithIndex\n",
        "rdd_with_values = df.rdd.zipWithIndex().map(\n",
        "    lambda row_index: Row(**row_index[0].asDict(), Country=country_list[row_index[1]])\n",
        ")\n",
        "\n",
        "# Create a new DataFrame with the additional column\n",
        "df_with_country = spark.createDataFrame(rdd_with_values)\n",
        "\n",
        "df_with_country.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2TMRTl-7vq_h",
        "outputId": "5e4917c6-26e9-4564-d81c-07ca409bd176"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+---+-------+\n",
            "| Name|Age|Country|\n",
            "+-----+---+-------+\n",
            "|Alice| 25|  India|\n",
            "|  Bob| 30|  China|\n",
            "|Cathy| 28|     UK|\n",
            "+-----+---+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "df.rdd: This converts the DataFrame df into an RDD (Resilient Distributed Dataset). An RDD is a low-level distributed collection of objects in PySpark.\n",
        "zipWithIndex(): This RDD operation returns a new RDD where each element is a tuple containing the original element and its index. For example, if the original RDD contains [(1, 2), (3, 4)], applying zipWithIndex() will result in [((1, 2), 0), ((3, 4), 1)].\n",
        "map(lambda row_index: Row(**row_index[0].asDict(), Country=country_list[row_index[1]])): This operation processes each row from the zipWithIndex result:\n",
        "\n",
        "    row_index[0]: This refers to the actual row data (e.g., (\"Alice\", 25)).\n",
        "    asDict(): Converts the Row object into a dictionary (e.g., {\"Name\": \"Alice\", \"Age\": 25}).\n",
        "    Country=country_list[row_index[1]]: This adds a new key-value pair (Country=country_name) to the dictionary. The value is taken from the country_list, using the index row_index[1] to match the row with a corresponding country from the list.\n",
        "    Row(**...): This constructs a new Row object with the combined fields from the original data and the new country value.\n",
        "\n",
        "The result is an RDD of Row objects, where each row now includes the original columns (Name, Age) and the new column (Country)."
      ],
      "metadata": {
        "id": "7SGhY-Cv6BaM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Qoz-PFoQ5_o-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Initialize SparkSession\n",
        "spark = SparkSession.builder.appName(\"DataFrameExample\").getOrCreate()\n",
        "\n",
        "# Create DataFrame from a list of tuples\n",
        "data = [(\"Alice\", 25), (\"Bob\", 30), (\"Cathy\", 28)]\n",
        "columns = [\"Name\", \"Age\"]\n",
        "df = spark.createDataFrame(data, columns)\n",
        "\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zi9CTrDdxEyg",
        "outputId": "88402bda-a5f2-481f-e5a4-09f6a3d7d3b2"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+---+\n",
            "| Name|Age|\n",
            "+-----+---+\n",
            "|Alice| 25|\n",
            "|  Bob| 30|\n",
            "|Cathy| 28|\n",
            "+-----+---+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = [{\"Name\": \"Alice\", \"Age\": 25}, {\"Name\": \"Bob\", \"Age\": 30}, {\"Name\": \"Cathy\", \"Age\": 28}]\n",
        "df = spark.createDataFrame(data)\n",
        "\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PaRq3QFLxuFe",
        "outputId": "d36e8073-ded7-4ceb-e111-77470a9968a3"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-----+\n",
            "|Age| Name|\n",
            "+---+-----+\n",
            "| 25|Alice|\n",
            "| 30|  Bob|\n",
            "| 28|Cathy|\n",
            "+---+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an RDD\n",
        "rdd = spark.sparkContext.parallelize([(\"Alice\", 25), (\"Bob\", 30), (\"Cathy\", 28)])\n",
        "\n",
        "# Convert RDD to DataFrame\n",
        "df = rdd.toDF([\"Name\", \"Age\"])\n",
        "\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bY4e5KElxyHB",
        "outputId": "2ee1d69f-aedf-4716-c878-e292695f4fe0"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+---+\n",
            "| Name|Age|\n",
            "+-----+---+\n",
            "|Alice| 25|\n",
            "|  Bob| 30|\n",
            "|Cathy| 28|\n",
            "+-----+---+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Create a Pandas DataFrame\n",
        "pandas_df = pd.DataFrame({\"Name\": [\"Alice\", \"Bob\", \"Cathy\"], \"Age\": [25, 30, 28]})\n",
        "\n",
        "# Convert Pandas DataFrame to PySpark DataFrame\n",
        "df = spark.createDataFrame(pandas_df)\n",
        "\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YbL8KZ6bx2JZ",
        "outputId": "297ac575-d79f-44af-c45e-8ce7af273b6e"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+---+\n",
            "| Name|Age|\n",
            "+-----+---+\n",
            "|Alice| 25|\n",
            "|  Bob| 30|\n",
            "|Cathy| 28|\n",
            "+-----+---+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### RDD(Resilient Distributed System)"
      ],
      "metadata": {
        "id": "yhrhGIKq0T3h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Create a SparkSession\n",
        "spark = SparkSession \\\n",
        "    .builder \\\n",
        "    .appName(\"Python Spark create RDD example\") \\\n",
        "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Create an RDD using parallelize\n",
        "rdd = spark.sparkContext.parallelize([\n",
        "    (1, 2, 3, 'a b c'),\n",
        "    (4, 5, 6, 'd e f'),\n",
        "    (7, 8, 9, 'g h i')\n",
        "])\n",
        "\n",
        "# Convert the RDD to a DataFrame with column names\n",
        "df = rdd.toDF(['col1', 'col2', 'col3', 'col4'])\n",
        "\n",
        "# Show the DataFrame\n",
        "df.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "43-o042vx57p",
        "outputId": "96bc0c54-0211-499d-b9be-c37b33f5aab2"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+----+----+-----+\n",
            "|col1|col2|col3| col4|\n",
            "+----+----+----+-----+\n",
            "|   1|   2|   3|a b c|\n",
            "|   4|   5|   6|d e f|\n",
            "|   7|   8|   9|g h i|\n",
            "+----+----+----+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "myData = spark.sparkContext.parallelize([(1,2), (3,4), (5,6), (7,8), (9,10)])\n",
        "myData.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L_6t31220aK8",
        "outputId": "5ccfa817-957d-4d54-ed55-a8345f6e4a91"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(1, 2), (3, 4), (5, 6), (7, 8), (9, 10)]"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PDU7R1OQ1wcw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}