{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fcf61427",
   "metadata": {},
   "source": [
    "### 1. Introduction to PySpark\n",
    "\n",
    "PySpark is the Python API for Apache Spark, an open-source distributed computing framework for big data processing. It enables Python users to leverage Sparkâ€™s powerful capabilities to process and analyze large datasets efficiently.<br>\n",
    "#### Why Use PySpark?<br>\n",
    "\n",
    "   - Handles large-scale data efficiently.\n",
    "   - Provides fault tolerance using RDDs (Resilient Distributed Datasets).\n",
    "   - Supports distributed computing across multiple machines.\n",
    "   - Compatible with SQL, Streaming, Machine Learning, and Graph Processing.\n",
    "   - Works with various data sources (HDFS, MySQL, S3, Cassandra, etc.)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e404dff",
   "metadata": {},
   "source": [
    "### Installing and Setting Up PySpark\n",
    "\n",
    "To install PySpark, use the following command:<br>\n",
    "`pip install pyspark`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c0c89273",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e6b8ed14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Adam</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bob</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Jennifer</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Alice</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Name  Age\n",
       "0      Adam   25\n",
       "1       Bob   28\n",
       "2  Jennifer   29\n",
       "3     Alice   22"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.read_excel(r'C:\\Users\\dell\\Desktop\\demo.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "995a3516",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7752b736",
   "metadata": {},
   "source": [
    "- `SparkSession` is the entry point for working with DataFrames and SQL in PySpark.\n",
    "- It provides methods for reading data, creating DataFrames, running SQL queries, and managing Spark configurations.\n",
    "- It replaces the older `SparkContext` and `SQLContext` used in early versions of PySpark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "be97d648",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "spark=SparkSession.builder.appName('Practice').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb06a541",
   "metadata": {},
   "source": [
    "This line creates or retrieves an existing Spark session. Let's break it down:\n",
    "\n",
    "   - `SparkSession.builder:`\n",
    "        It is used to configure and initialize a new Spark session.<br>\n",
    "\n",
    "   - `.appName(\"Practice\"):`\n",
    "        Assigns the application name as \"Practice\", which is useful for identifying the job in the Spark UI.<br>\n",
    "        If you run multiple Spark jobs, giving them unique names can help with debugging.<br>\n",
    "\n",
    "   - `.getOrCreate():`\n",
    "        Checks if a Spark session already exists.<br>\n",
    "        If an existing session is found, it returns that session instead of creating a new one.<br>\n",
    "        If no session exists, it creates a new Spark session.\n",
    "\n",
    "#### ðŸ’¡ Why use .getOrCreate()?\n",
    "\n",
    "    If you run the same script multiple times in an interactive environment (e.g., Jupyter Notebook), it avoids creating multiple Spark sessions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2b4a6d8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://DESKTOP-SB35PE4.mshome.net:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Practice</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x16d56d5e110>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e9630831",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark=spark.read.csv(r'C:\\Users\\dell\\Desktop\\demo.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e3d305c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[_c0: string, _c1: string, _c2: string]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c1ddf808",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---+----------+\n",
      "|     _c0|_c1|       _c2|\n",
      "+--------+---+----------+\n",
      "|    Name|Age|Experience|\n",
      "|    Adam| 25|         4|\n",
      "|     Bob| 28|         2|\n",
      "|Jennifer| 29|         1|\n",
      "|   Alice| 22|         3|\n",
      "+--------+---+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7877b34f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark=spark.read.option('header','true').csv(r'C:\\Users\\dell\\Desktop\\demo.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a63631de",
   "metadata": {},
   "source": [
    "| Code Snippet | Explanation |\n",
    "|-------------|------------|\n",
    "| `df_pyspark =` | Stores the output DataFrame in `df_pyspark`. |\n",
    "| `spark.read` | Accesses the DataFrameReader to read data. |\n",
    "| `.option('header', 'true')` | Treats the first row as column headers. |\n",
    "| `.csv(r'C:\\Users\\dell\\Desktop\\demo.csv')` | Reads the CSV file from the specified path. |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8c44799c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df_pyspark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "10479132",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Name='Adam', Age='25', Experience='4'),\n",
       " Row(Name='Bob', Age='28', Experience='2'),\n",
       " Row(Name='Jennifer', Age='29', Experience='1')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pyspark.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e6690ffe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Age: string (nullable = true)\n",
      " |-- Experience: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# check the schema\n",
    "df_pyspark.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8ac59a7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Name: string, Age: string, Experience: string]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9f500038",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark=spark.read.csv(r'C:\\Users\\dell\\Desktop\\demo.csv',header=True,inferSchema=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83fc01a8",
   "metadata": {},
   "source": [
    "| Code Snippet | Explanation |\n",
    "|-------------|------------|\n",
    "| `df_pyspark =` | Stores the output DataFrame in `df_pyspark`. |\n",
    "| `spark.read.csv()` | Reads the CSV file into a DataFrame. |\n",
    "| `r'C:\\Users\\dell\\Desktop\\demo.csv'` | Specifies the file path (raw string format for Windows). |\n",
    "| `header=True` | Treats the first row as column headers. |\n",
    "| `inferSchema=True` | Automatically detects column data types instead of reading everything as strings. |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1b2b6dff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---+----------+\n",
      "|    Name|Age|Experience|\n",
      "+--------+---+----------+\n",
      "|    Adam| 25|         4|\n",
      "|     Bob| 28|         2|\n",
      "|Jennifer| 29|         1|\n",
      "|   Alice| 22|         3|\n",
      "+--------+---+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41544f2f",
   "metadata": {},
   "source": [
    "## PySpark DataFrame Operations\n",
    "\n",
    "| Code Snippet | Explanation |\n",
    "|-------------|------------|\n",
    "| `df_pyspark.show()` | Displays the first 20 rows of the DataFrame in a tabular format. |\n",
    "| `df_pyspark.printSchema()` | Prints the schema of the DataFrame, showing column names and data types. |\n",
    "| `df_pyspark.columns` | Returns a list of all column names in the DataFrame. |\n",
    "| `df_pyspark.describe().show()` | Displays summary statistics (count, mean, stddev, min, max) for numerical columns. |\n",
    "| `df_pyspark.head()` | Returns the first row of the DataFrame as a `Row` object. |\n",
    "| `df_pyspark.head(5)` | Returns the first 5 rows as a list of `Row` objects. |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0c815652",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Age: integer (nullable = true)\n",
      " |-- Experience: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# check the schema\n",
    "df_pyspark.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d3470f9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df_pyspark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "15095450",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Name='Adam', Age=25, Experience=4),\n",
       " Row(Name='Bob', Age=28, Experience=2),\n",
       " Row(Name='Jennifer', Age=29, Experience=1)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# it returns list of tuple but pandas return dataframe\n",
    "df_pyspark.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "de204b3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---+----------+\n",
      "|    Name|Age|Experience|\n",
      "+--------+---+----------+\n",
      "|    Adam| 25|         4|\n",
      "|     Bob| 28|         2|\n",
      "|Jennifer| 29|         1|\n",
      "|   Alice| 22|         3|\n",
      "+--------+---+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8f80ce56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|    Name|\n",
      "+--------+\n",
      "|    Adam|\n",
      "|     Bob|\n",
      "|Jennifer|\n",
      "|   Alice|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# filter the dataframe using column\n",
    "df_pyspark.select('Name').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "58f4703f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---+\n",
      "|    Name|Age|\n",
      "+--------+---+\n",
      "|    Adam| 25|\n",
      "|     Bob| 28|\n",
      "|Jennifer| 29|\n",
      "|   Alice| 22|\n",
      "+--------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.select(['Name','Age']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "62ed4d3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Name', 'string'), ('Age', 'int'), ('Experience', 'int')]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pyspark.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8e9c8220",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+-----------------+------------------+\n",
      "|summary|    Name|              Age|        Experience|\n",
      "+-------+--------+-----------------+------------------+\n",
      "|  count|       4|                4|                 4|\n",
      "|   mean|    NULL|             26.0|               2.5|\n",
      "| stddev|    NULL|3.162277660168379|1.2909944487358056|\n",
      "|    min|    Adam|               22|                 1|\n",
      "|    max|Jennifer|               29|                 4|\n",
      "+-------+--------+-----------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dc071353",
   "metadata": {},
   "outputs": [],
   "source": [
    "### adding columns in dataframe\n",
    "df_pyspark=df_pyspark.withColumn('Experience After 2 Years',df_pyspark['Experience']+2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b4a55731",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---+----------+------------------------+\n",
      "|    Name|Age|Experience|Experience After 2 Years|\n",
      "+--------+---+----------+------------------------+\n",
      "|    Adam| 25|         4|                       6|\n",
      "|     Bob| 28|         2|                       4|\n",
      "|Jennifer| 29|         1|                       3|\n",
      "|   Alice| 22|         3|                       5|\n",
      "+--------+---+----------+------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c3b06829",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark=df_pyspark.drop('Experience After 2 Years')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c06f09c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---+----------+\n",
      "|    Name|Age|Experience|\n",
      "+--------+---+----------+\n",
      "|    Adam| 25|         4|\n",
      "|     Bob| 28|         2|\n",
      "|Jennifer| 29|         1|\n",
      "|   Alice| 22|         3|\n",
      "+--------+---+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4ba1810b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark=df_pyspark.withColumnRenamed('Name','New Name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "51fc95b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---+----------+\n",
      "|New Name|Age|Experience|\n",
      "+--------+---+----------+\n",
      "|    Adam| 25|         4|\n",
      "|     Bob| 28|         2|\n",
      "|Jennifer| 29|         1|\n",
      "|   Alice| 22|         3|\n",
      "+--------+---+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "48dba4ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test=spark.read.csv(r'C:\\Users\\dell\\Desktop\\test.csv',header=True,inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fbc7437c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Name: string, Age: int, Experience: int, Salary: int]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8a373953",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+----------+------+\n",
      "|     Name| Age|Experience|Salary|\n",
      "+---------+----+----------+------+\n",
      "|    Krish|  31|        10| 30000|\n",
      "|Sudhanshu|  30|         8| 25000|\n",
      "|    Sunny|  29|         4| 20000|\n",
      "|     Paul|  24|         3| 20000|\n",
      "|   Harsha|  21|         1| 15000|\n",
      "|  Shubham|  23|         2| 18000|\n",
      "|   Mahesh|  40|        10| 40000|\n",
      "|     Amit|NULL|      NULL| 45000|\n",
      "|     NULL|  36|        10| 60000|\n",
      "|     NULL|  34|      NULL|  NULL|\n",
      "+---------+----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_test.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9c1df066",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+------+\n",
      "| Age|Experience|Salary|\n",
      "+----+----------+------+\n",
      "|  31|        10| 30000|\n",
      "|  30|         8| 25000|\n",
      "|  29|         4| 20000|\n",
      "|  24|         3| 20000|\n",
      "|  21|         1| 15000|\n",
      "|  23|         2| 18000|\n",
      "|  40|        10| 40000|\n",
      "|NULL|      NULL| 45000|\n",
      "|  36|        10| 60000|\n",
      "|  34|      NULL|  NULL|\n",
      "+----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# drop the column\n",
    "df_test.drop('Name').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e6e9a250",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+----------+------+\n",
      "|     Name| Age|Experience|Salary|\n",
      "+---------+----+----------+------+\n",
      "|    Krish|  31|        10| 30000|\n",
      "|Sudhanshu|  30|         8| 25000|\n",
      "|    Sunny|  29|         4| 20000|\n",
      "|     Paul|  24|         3| 20000|\n",
      "|   Harsha|  21|         1| 15000|\n",
      "|  Shubham|  23|         2| 18000|\n",
      "|   Mahesh|  40|        10| 40000|\n",
      "|     Amit|NULL|      NULL| 45000|\n",
      "|     NULL|  36|        10| 60000|\n",
      "|     NULL|  34|      NULL|  NULL|\n",
      "+---------+----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_test.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "449846bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+----------+------+\n",
      "|     Name|Age|Experience|Salary|\n",
      "+---------+---+----------+------+\n",
      "|    Krish| 31|        10| 30000|\n",
      "|Sudhanshu| 30|         8| 25000|\n",
      "|    Sunny| 29|         4| 20000|\n",
      "|     Paul| 24|         3| 20000|\n",
      "|   Harsha| 21|         1| 15000|\n",
      "|  Shubham| 23|         2| 18000|\n",
      "|   Mahesh| 40|        10| 40000|\n",
      "+---------+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_test.na.drop().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6f16bad5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+----------+------+\n",
      "|     Name| Age|Experience|Salary|\n",
      "+---------+----+----------+------+\n",
      "|    Krish|  31|        10| 30000|\n",
      "|Sudhanshu|  30|         8| 25000|\n",
      "|    Sunny|  29|         4| 20000|\n",
      "|     Paul|  24|         3| 20000|\n",
      "|   Harsha|  21|         1| 15000|\n",
      "|  Shubham|  23|         2| 18000|\n",
      "|   Mahesh|  40|        10| 40000|\n",
      "|     Amit|NULL|      NULL| 45000|\n",
      "|     NULL|  36|        10| 60000|\n",
      "|     NULL|  34|      NULL|  NULL|\n",
      "+---------+----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# any==how\n",
    "df_test.na.drop(how='all').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5118825b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+----------+------+\n",
      "|     Name|Age|Experience|Salary|\n",
      "+---------+---+----------+------+\n",
      "|    Krish| 31|        10| 30000|\n",
      "|Sudhanshu| 30|         8| 25000|\n",
      "|    Sunny| 29|         4| 20000|\n",
      "|     Paul| 24|         3| 20000|\n",
      "|   Harsha| 21|         1| 15000|\n",
      "|  Shubham| 23|         2| 18000|\n",
      "|   Mahesh| 40|        10| 40000|\n",
      "+---------+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# any==how\n",
    "df_test.na.drop(how='any').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ba849934",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+----------+------+\n",
      "|     Name|Age|Experience|Salary|\n",
      "+---------+---+----------+------+\n",
      "|    Krish| 31|        10| 30000|\n",
      "|Sudhanshu| 30|         8| 25000|\n",
      "|    Sunny| 29|         4| 20000|\n",
      "|     Paul| 24|         3| 20000|\n",
      "|   Harsha| 21|         1| 15000|\n",
      "|  Shubham| 23|         2| 18000|\n",
      "|   Mahesh| 40|        10| 40000|\n",
      "|     NULL| 36|        10| 60000|\n",
      "+---------+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# thresold\n",
    "df_test.na.drop(how='all',thresh=3).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "640db5da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+----------+------+\n",
      "|     Name| Age|Experience|Salary|\n",
      "+---------+----+----------+------+\n",
      "|    Krish|  31|        10| 30000|\n",
      "|Sudhanshu|  30|         8| 25000|\n",
      "|    Sunny|  29|         4| 20000|\n",
      "|     Paul|  24|         3| 20000|\n",
      "|   Harsha|  21|         1| 15000|\n",
      "|  Shubham|  23|         2| 18000|\n",
      "|   Mahesh|  40|        10| 40000|\n",
      "|     Amit|NULL|      NULL| 45000|\n",
      "|     NULL|  36|        10| 60000|\n",
      "+---------+----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# thresold\n",
    "df_test.na.drop(how='all',subset=['Experience','Salary']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b76a7883",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+----------+------+\n",
      "|     Name| Age|Experience|Salary|\n",
      "+---------+----+----------+------+\n",
      "|    Krish|  31|        10| 30000|\n",
      "|Sudhanshu|  30|         8| 25000|\n",
      "|    Sunny|  29|         4| 20000|\n",
      "|     Paul|  24|         3| 20000|\n",
      "|   Harsha|  21|         1| 15000|\n",
      "|  Shubham|  23|         2| 18000|\n",
      "|   Mahesh|  40|        10| 40000|\n",
      "|     Amit|NULL|      NULL| 45000|\n",
      "|     NULL|  36|        10| 60000|\n",
      "|     NULL|  34|      NULL|  NULL|\n",
      "+---------+----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# filling the null values\n",
    "df_test.na.fill('Missing Values','Experience').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "379bc6df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+----------+------+\n",
      "|     Name|Age|Experience|Salary|\n",
      "+---------+---+----------+------+\n",
      "|    Krish| 31|        10| 30000|\n",
      "|Sudhanshu| 30|         8| 25000|\n",
      "|    Sunny| 29|         4| 20000|\n",
      "|     Paul| 24|         3| 20000|\n",
      "|   Harsha| 21|         1| 15000|\n",
      "|  Shubham| 23|         2| 18000|\n",
      "|   Mahesh| 40|        10| 40000|\n",
      "|     Amit| 50|        50| 45000|\n",
      "|     NULL| 36|        10| 60000|\n",
      "|     NULL| 34|        50|    50|\n",
      "+---------+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_test.na.fill(50).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f11d34af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+----------+------+\n",
      "|     Name|Age|Experience|Salary|\n",
      "+---------+---+----------+------+\n",
      "|    Krish| 31|        10| 30000|\n",
      "|Sudhanshu| 30|         8| 25000|\n",
      "|    Sunny| 29|         4| 20000|\n",
      "|     Paul| 24|         3| 20000|\n",
      "|   Harsha| 21|         1| 15000|\n",
      "|  Shubham| 23|         2| 18000|\n",
      "|   Mahesh| 40|        10| 40000|\n",
      "|     Amit| 50|        11| 45000|\n",
      "|  unknown| 36|        10| 60000|\n",
      "|  unknown| 34|        11|100000|\n",
      "+---------+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_test.na.fill({'Age': 50, 'Name': 'unknown','Salary':100000,'Experience':11}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9625e1b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace missing value with the mean of column\n",
    "from pyspark.ml.feature import Imputer\n",
    "\n",
    "imputer=Imputer(\n",
    "    inputCols=['Age','Experience','Salary'],\n",
    "    outputCols=[\"{}_imputed\".format(c) for c in ['Age','Experience','Salary']]\n",
    "\n",
    ").setStrategy(\"mean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "15226aa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+----------+------+-----------+------------------+--------------+\n",
      "|     Name| Age|Experience|Salary|Age_imputed|Experience_imputed|Salary_imputed|\n",
      "+---------+----+----------+------+-----------+------------------+--------------+\n",
      "|    Krish|  31|        10| 30000|         31|                10|         30000|\n",
      "|Sudhanshu|  30|         8| 25000|         30|                 8|         25000|\n",
      "|    Sunny|  29|         4| 20000|         29|                 4|         20000|\n",
      "|     Paul|  24|         3| 20000|         24|                 3|         20000|\n",
      "|   Harsha|  21|         1| 15000|         21|                 1|         15000|\n",
      "|  Shubham|  23|         2| 18000|         23|                 2|         18000|\n",
      "|   Mahesh|  40|        10| 40000|         40|                10|         40000|\n",
      "|     Amit|NULL|      NULL| 45000|         29|                 6|         45000|\n",
      "|     NULL|  36|        10| 60000|         36|                10|         60000|\n",
      "|     NULL|  34|      NULL|  NULL|         34|                 6|         30333|\n",
      "+---------+----+----------+------+-----------+------------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# add imputation cols to df\n",
    "imputer.fit(df_test).transform(df_test).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "653ec0b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+----------+------+\n",
      "|     Name| Age|Experience|Salary|\n",
      "+---------+----+----------+------+\n",
      "|    Krish|  31|        10| 30000|\n",
      "|Sudhanshu|  30|         8| 25000|\n",
      "|    Sunny|  29|         4| 20000|\n",
      "|     Paul|  24|         3| 20000|\n",
      "|   Harsha|  21|         1| 15000|\n",
      "|  Shubham|  23|         2| 18000|\n",
      "|   Mahesh|  40|        10| 40000|\n",
      "|     Amit|NULL|      NULL| 45000|\n",
      "|  Unknown|  36|        10| 60000|\n",
      "|  Unknown|  34|      NULL|  NULL|\n",
      "+---------+----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_test.fillna({\"Name\": \"Unknown\"}).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4836330e",
   "metadata": {},
   "source": [
    "### Filter Operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "df0529bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test=df_test.na.drop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "805a641e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+----------+------+\n",
      "|     Name|Age|Experience|Salary|\n",
      "+---------+---+----------+------+\n",
      "|    Krish| 31|        10| 30000|\n",
      "|Sudhanshu| 30|         8| 25000|\n",
      "|    Sunny| 29|         4| 20000|\n",
      "|     Paul| 24|         3| 20000|\n",
      "|   Harsha| 21|         1| 15000|\n",
      "|  Shubham| 23|         2| 18000|\n",
      "|   Mahesh| 40|        10| 40000|\n",
      "+---------+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_test.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "6a7efdd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+----------+------+\n",
      "|     Name|Age|Experience|Salary|\n",
      "+---------+---+----------+------+\n",
      "|Sudhanshu| 30|         8| 25000|\n",
      "|    Sunny| 29|         4| 20000|\n",
      "|     Paul| 24|         3| 20000|\n",
      "|   Harsha| 21|         1| 15000|\n",
      "|  Shubham| 23|         2| 18000|\n",
      "+---------+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# salary of people less than 25000\n",
    "df_test.filter(\"Salary<=25000\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1fa04901",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+\n",
      "|     Name|Age|\n",
      "+---------+---+\n",
      "|Sudhanshu| 30|\n",
      "|    Sunny| 29|\n",
      "|     Paul| 24|\n",
      "|   Harsha| 21|\n",
      "|  Shubham| 23|\n",
      "+---------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_test.filter(\"Salary<=25000\").select(['Name','Age']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c8ea9323",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+----------+------+\n",
      "|     Name|Age|Experience|Salary|\n",
      "+---------+---+----------+------+\n",
      "|Sudhanshu| 30|         8| 25000|\n",
      "|    Sunny| 29|         4| 20000|\n",
      "|     Paul| 24|         3| 20000|\n",
      "|   Harsha| 21|         1| 15000|\n",
      "|  Shubham| 23|         2| 18000|\n",
      "+---------+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_test.filter(df_test['Salary']<=25000).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "bfe2ff1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+----------+------+\n",
      "|     Name|Age|Experience|Salary|\n",
      "+---------+---+----------+------+\n",
      "|Sudhanshu| 30|         8| 25000|\n",
      "|    Sunny| 29|         4| 20000|\n",
      "+---------+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_test.filter((df_test['Salary']<=25000) & (df_test['Age']>=25)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac77ced5",
   "metadata": {},
   "source": [
    "### Pyspark GroupBy and Aggregate Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "6b39ed76",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_data=spark.read.csv(r'C:\\Users\\dell\\Downloads\\sample_data.csv',header=True,inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e9b2b3cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+------+-----+--------+------+\n",
      "|   Category|Subcategory|Region|Sales|Quantity|Profit|\n",
      "+-----------+-----------+------+-----+--------+------+\n",
      "|Electronics|     Mobile| North|  500|       5|    50|\n",
      "|Electronics|     Laptop| South| 1500|      10|   200|\n",
      "|  Furniture|      Chair| North|  200|       2|    20|\n",
      "|  Furniture|      Table|  East|  300|       3|    40|\n",
      "|   Clothing|      Shirt|  West|  100|       1|    10|\n",
      "|   Clothing|      Jeans|  West|  150|       2|    15|\n",
      "|Electronics|     Mobile| South|  700|       7|    70|\n",
      "+-----------+-----------+------+-----+--------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sample_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "9c59d284",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Category: string (nullable = true)\n",
      " |-- Subcategory: string (nullable = true)\n",
      " |-- Region: string (nullable = true)\n",
      " |-- Sales: integer (nullable = true)\n",
      " |-- Quantity: integer (nullable = true)\n",
      " |-- Profit: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sample_data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b23b09bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+-------------+-----------+\n",
      "|   Category|sum(Sales)|sum(Quantity)|sum(Profit)|\n",
      "+-----------+----------+-------------+-----------+\n",
      "|Electronics|      2700|           22|        320|\n",
      "|   Clothing|       250|            3|         25|\n",
      "|  Furniture|       500|            5|         60|\n",
      "+-----------+----------+-------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#### GroupBy\n",
    "sample_data.groupBy('Category').sum().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "57a89a00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+-------------+-----------+\n",
      "|   Category|sum(Sales)|sum(Quantity)|sum(Profit)|\n",
      "+-----------+----------+-------------+-----------+\n",
      "|Electronics|      2700|           22|        320|\n",
      "|   Clothing|       250|            3|         25|\n",
      "|  Furniture|       500|            5|         60|\n",
      "+-----------+----------+-------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sample_data.groupBy('Category').sum().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "7d583c00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+\n",
      "|   Category|sum(Sales)|\n",
      "+-----------+----------+\n",
      "|Electronics|      2700|\n",
      "|   Clothing|       250|\n",
      "|  Furniture|       500|\n",
      "+-----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sample_data.groupBy('Category').agg({'Sales': 'sum'}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "982742f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+-------------+------------------+\n",
      "|   Category|sum(Sales)|max(Quantity)|       avg(Profit)|\n",
      "+-----------+----------+-------------+------------------+\n",
      "|Electronics|      2700|           10|106.66666666666667|\n",
      "|   Clothing|       250|            2|              12.5|\n",
      "|  Furniture|       500|            3|              30.0|\n",
      "+-----------+----------+-------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sample_data.groupBy('Category').agg({'Sales':'sum','Profit':'mean','Quantity':'max'}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "66782955",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+------+-----+--------+------+\n",
      "|   Category|Subcategory|Region|Sales|Quantity|Profit|\n",
      "+-----------+-----------+------+-----+--------+------+\n",
      "|Electronics|     Mobile| North|  500|       5|    50|\n",
      "|Electronics|     Laptop| South| 1500|      10|   200|\n",
      "|Electronics|     Mobile| South|  700|       7|    70|\n",
      "+-----------+-----------+------+-----+--------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sample_data.filter(sample_data['Category']=='Electronics').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "1163203c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows: 7\n"
     ]
    }
   ],
   "source": [
    "# Count the number of rows\n",
    "print(\"Number of rows:\", sample_data.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "821701d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|   Category|\n",
      "+-----------+\n",
      "|Electronics|\n",
      "|   Clothing|\n",
      "|  Furniture|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Distinct categories\n",
    "sample_data.select(\"Category\").distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "a33a2f10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+------+-----+--------+------+\n",
      "|   Category|Subcategory|Region|Sales|Quantity|Profit|\n",
      "+-----------+-----------+------+-----+--------+------+\n",
      "|Electronics|     Mobile| North|  500|       5|    50|\n",
      "|Electronics|     Laptop| South| 1500|      10|   200|\n",
      "|Electronics|     Mobile| South|  700|       7|    70|\n",
      "+-----------+-----------+------+-----+--------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sample_data.filter(col(\"Category\") == \"Electronics\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "8cff17bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+------+-----+--------+------+---+\n",
      "|   Category|Subcategory|Region|Sales|Quantity|Profit|Tax|\n",
      "+-----------+-----------+------+-----+--------+------+---+\n",
      "|Electronics|     Mobile| North|  500|       5|    50| 10|\n",
      "|Electronics|     Laptop| South| 1500|      10|   200| 10|\n",
      "|  Furniture|      Chair| North|  200|       2|    20| 10|\n",
      "|  Furniture|      Table|  East|  300|       3|    40| 10|\n",
      "|   Clothing|      Shirt|  West|  100|       1|    10| 10|\n",
      "|   Clothing|      Jeans|  West|  150|       2|    15| 10|\n",
      "|Electronics|     Mobile| South|  700|       7|    70| 10|\n",
      "+-----------+-----------+------+-----+--------+------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Add a new column with a constant value\n",
    "sample_data =sample_data.withColumn(\"Tax\", lit(10))\n",
    "sample_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "51deccb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+------+-----+--------+------+---+----------+\n",
      "|   Category|Subcategory|Region|Sales|Quantity|Profit|Tax|Net_Profit|\n",
      "+-----------+-----------+------+-----+--------+------+---+----------+\n",
      "|Electronics|     Mobile| North|  500|       5|    50| 10|        40|\n",
      "|Electronics|     Laptop| South| 1500|      10|   200| 10|       190|\n",
      "|  Furniture|      Chair| North|  200|       2|    20| 10|        10|\n",
      "|  Furniture|      Table|  East|  300|       3|    40| 10|        30|\n",
      "|   Clothing|      Shirt|  West|  100|       1|    10| 10|         0|\n",
      "|   Clothing|      Jeans|  West|  150|       2|    15| 10|         5|\n",
      "|Electronics|     Mobile| South|  700|       7|    70| 10|        60|\n",
      "+-----------+-----------+------+-----+--------+------+---+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Add a column with a calculated value\n",
    "sample_data= sample_data.withColumn(\"Net_Profit\", col(\"Profit\") - col(\"Tax\"))\n",
    "sample_data.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2adca75e",
   "metadata": {},
   "source": [
    "### Sorting Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "5270b378",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+------+-----+--------+------+---+----------+----------+\n",
      "|   Category|Subcategory|Region|Sales|Quantity|Profit|Tax|Net_Profit|High_Sales|\n",
      "+-----------+-----------+------+-----+--------+------+---+----------+----------+\n",
      "|   Clothing|      Shirt|  West|  100|       1|    10| 10|         0|        No|\n",
      "|   Clothing|      Jeans|  West|  150|       2|    15| 10|         5|        No|\n",
      "|  Furniture|      Chair| North|  200|       2|    20| 10|        10|        No|\n",
      "|  Furniture|      Table|  East|  300|       3|    40| 10|        30|        No|\n",
      "|Electronics|     Mobile| North|  500|       5|    50| 10|        40|        No|\n",
      "|Electronics|     Mobile| South|  700|       7|    70| 10|        60|       Yes|\n",
      "|Electronics|     Laptop| South| 1500|      10|   200| 10|       190|       Yes|\n",
      "+-----------+-----------+------+-----+--------+------+---+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Sort by Sales (ascending)\n",
    "sample_data.orderBy(\"Sales\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "c0868933",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+------+-----+--------+------+---+----------+----------+\n",
      "|   Category|Subcategory|Region|Sales|Quantity|Profit|Tax|Net_Profit|High_Sales|\n",
      "+-----------+-----------+------+-----+--------+------+---+----------+----------+\n",
      "|Electronics|     Laptop| South| 1500|      10|   200| 10|       190|       Yes|\n",
      "|Electronics|     Mobile| South|  700|       7|    70| 10|        60|       Yes|\n",
      "|Electronics|     Mobile| North|  500|       5|    50| 10|        40|        No|\n",
      "|  Furniture|      Table|  East|  300|       3|    40| 10|        30|        No|\n",
      "|  Furniture|      Chair| North|  200|       2|    20| 10|        10|        No|\n",
      "|   Clothing|      Jeans|  West|  150|       2|    15| 10|         5|        No|\n",
      "|   Clothing|      Shirt|  West|  100|       1|    10| 10|         0|        No|\n",
      "+-----------+-----------+------+-----+--------+------+---+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Sort by Sales (descending)\n",
    "sample_data.orderBy(desc(\"Sales\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60f75273",
   "metadata": {},
   "source": [
    "## Aggregations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "86bc8b58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+\n",
      "|   Category|sum(Sales)|\n",
      "+-----------+----------+\n",
      "|Electronics|      2700|\n",
      "|   Clothing|       250|\n",
      "|  Furniture|       500|\n",
      "+-----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Group by Category and sum Sales\n",
    "sample_data.groupBy(\"Category\").sum(\"Sales\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "f5134334",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------+\n",
      "|Region|avg(Profit)|\n",
      "+------+-----------+\n",
      "| South|      135.0|\n",
      "|  East|       40.0|\n",
      "|  West|       12.5|\n",
      "| North|       35.0|\n",
      "+------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Group by Region and calculate average Profit\n",
    "sample_data.groupBy(\"Region\").avg(\"Profit\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "5df4df3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------+-----------+----------+------------+\n",
      "|   Category|Region|Total_Sales|Avg_Profit|Max_Quantity|\n",
      "+-----------+------+-----------+----------+------------+\n",
      "|  Furniture|  East|        300|      40.0|           3|\n",
      "|   Clothing|  West|        250|      12.5|           2|\n",
      "|  Furniture| North|        200|      20.0|           2|\n",
      "|Electronics| North|        500|      50.0|           5|\n",
      "|Electronics| South|       2200|     135.0|          10|\n",
      "+-----------+------+-----------+----------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Group by Category and Region, and calculate multiple aggregates\n",
    "sample_data.groupBy(\"Category\", \"Region\").agg(\n",
    "    sum(\"Sales\").alias(\"Total_Sales\"),\n",
    "    avg(\"Profit\").alias(\"Avg_Profit\"),\n",
    "    max(\"Quantity\").alias(\"Max_Quantity\")\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "8b41f2e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+------+-----+--------+------+---+----------+----------+\n",
      "|   Category|Subcategory|Region|Sales|Quantity|Profit|Tax|Net_Profit|High_Sales|\n",
      "+-----------+-----------+------+-----+--------+------+---+----------+----------+\n",
      "|Electronics|     Mobile| North|  500|       5|    50| 10|        40|        No|\n",
      "|Electronics|     Laptop| South| 1500|      10|   200| 10|       190|       Yes|\n",
      "|  Furniture|      Chair| North|  200|       2|    20| 10|        10|        No|\n",
      "|  Furniture|      Table|  East|  300|       3|    40| 10|        30|        No|\n",
      "|   Clothing|      Shirt|  West|  100|       1|    10| 10|         0|        No|\n",
      "|   Clothing|      Jeans|  West|  150|       2|    15| 10|         5|        No|\n",
      "|Electronics|     Mobile| South|  700|       7|    70| 10|        60|       Yes|\n",
      "+-----------+-----------+------+-----+--------+------+---+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sample_data.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5029d9be",
   "metadata": {},
   "source": [
    "## Pivot and Unpivot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "38e39b1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+-----------+---------+\n",
      "|Region|Clothing|Electronics|Furniture|\n",
      "+------+--------+-----------+---------+\n",
      "| South|    NULL|       2200|     NULL|\n",
      "|  East|    NULL|       NULL|      300|\n",
      "|  West|     250|       NULL|     NULL|\n",
      "| North|    NULL|        500|      200|\n",
      "+------+--------+-----------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Pivot: Total Sales by Region and Category\n",
    "sample_data.groupBy(\"Region\").pivot(\"Category\").sum(\"Sales\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfc2a6ab",
   "metadata": {},
   "source": [
    "# **What is RDD in PySpark?**  \n",
    "\n",
    "RDD (**Resilient Distributed Dataset**) is the **fundamental data structure** in PySpark. It is an **immutable**, **distributed**, and **fault-tolerant** collection of objects that can be processed in parallel across multiple nodes in a cluster.  \n",
    "\n",
    "## **Key Features of RDD**  \n",
    "- **Immutable** â†’ Once created, it cannot be changed.  \n",
    "- **Distributed** â†’ Data is distributed across multiple machines.  \n",
    "- **Lazy Evaluation** â†’ Computations are performed only when an action is triggered.  \n",
    "- **Fault-Tolerant** â†’ Automatically recovers from failures.  \n",
    "- **Parallel Processing** â†’ Leverages multiple CPU cores and machines.  \n",
    "\n",
    "---\n",
    "\n",
    "## **RDD vs. Pandas DataFrame**  \n",
    "\n",
    "| Feature | RDD | Pandas DataFrame |\n",
    "|---------|-----|-----------------|\n",
    "| **Data Handling** | Works with unstructured and structured data. | Works mainly with structured data (tables). |\n",
    "| **Storage** | Distributed across multiple machines. | Stored in a single machine's memory. |\n",
    "| **Performance** | Optimized for big data but lacks built-in optimizations. | Fast for small to medium-sized data. |\n",
    "| **Operations** | Uses **map, filter, reduce** functions for transformations. | Uses **vectorized operations** (NumPy-based) for speed. |\n",
    "| **Lazy Evaluation** | Yes, computations run only when an action is called. | No, operations execute immediately. |\n",
    "| **Ease of Use** | Requires functional programming concepts. | Simple and user-friendly with intuitive syntax. |\n",
    "| **Schema Support** | No schema enforcement (unstructured data). | Enforces schema and supports mixed data types. |\n",
    "\n",
    "---\n",
    "\n",
    "## **Example Usage**\n",
    "### **Creating an RDD in PySpark**\n",
    "```python<br>\n",
    "`from pyspark.sql import SparkSession`\n",
    "\n",
    "### Initialize Spark Session\n",
    "`spark = SparkSession.builder.appName(\"RDD Example\").getOrCreate()`\n",
    "\n",
    "### Create an RDD from a Python list\n",
    "`rdd = spark.sparkContext.parallelize([(\"Alice\", 25), (\"Bob\", 30), (\"Charlie\", 35)])`\n",
    "\n",
    "### Display the RDD content\n",
    "`print(rdd.collect())`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d966d814",
   "metadata": {},
   "source": [
    "## Using Pandas\n",
    "`import pandas as pd`\n",
    "\n",
    "### Create a DataFrame\n",
    "`df = pd.DataFrame([(\"Alice\", 25), (\"Bob\", 30), (\"Charlie\", 35)], columns=[\"Name\", \"Age\"])`\n",
    "\n",
    "### Display the DataFrame\n",
    "`print(df)`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc159fe6",
   "metadata": {},
   "source": [
    "### When to Use RDD vs. Pandas?\n",
    "\n",
    "   - **Use RDD** when dealing with **large-scale distributed data processing**.\n",
    "   - Use **Pandas** for small to **medium-sized datasets** that fit in **memory.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b1e1a8c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
